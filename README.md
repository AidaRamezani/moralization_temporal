Please visit OSF to download the data: https://osf.io/kjynq/?view_only=f10e01eba9c54779b5de469e489a4e5b



### Historical reconstruction of human moralization

This repository contains code and data for replicating the following work: Ramezani, Stellar, Feinberg, and Xu. (submitted) Historical reconstruction of human moralization.



#### Adding your own data
If you want to reproduce this framework using your own diachronic data, instead of COHA or NYY, you need to follow the steps below:
1. **Data Preprocessing**: Add your own data reading functions inside ```SWOW_prediction/data_preprocessing_utils.py```. Your function should retrun List[str] where each element is an article/utterance/paragraph/... from a specific time point in your datasest. Look at the ```get_coha_data``` and ```get_nyt_data``` for sample implementations.

2. **Modify configuration**: Modify the ```SWOW_prediction/config_features.yml``` file to include your own data. You should modify ```data_name```, ```data_path```, and ```data_features``` according to your data specification. 

3. **Adding new scripts**: Add a script ```scripts/data_job_<your_data_name>.sh``` to run the data preprocessing and model training. You can use ```scripts/data_job_nyt.sh``` as a template. Make sure to adjust the ```SBATCH --array=1987-2007``` to be in the range of your data. Run this scripts using slurm job scheduler to preprocess your data and store it. 

4. **Training**: Similarly, modify the scripts ```scripts/training_job.sh``` and ```scripts/evaluation_job.sh``` and ```scripts/historical_inference.sh``` to include your own data. After modifying, run these scripts sequentially to train your model and evaluate it on your data. Your results will be saved in the ```data/SWOW_prediction/eval/time_series``` directory.



#### Working with COHA and NYT datasets

The main datasets created by our framework are available at 
```ts_df.csv``` for COHA analysis, and ```nyt_ts_df.csv``` for NYT analysis. Place them inside the ```data``` directory if you wish to replicate the experiments reported in the paper.

The datasets provide the following information:
```("words", "year","property", "section", "outputs_z", and "outputs")```, where ```property``` is a determines the moral association score generated by our framework at the specific ```year``` by the model indexed at ```section```. The variable ```outputs_z``` are the z-score standardized moral association scores, which we have reported throughout the paper, but the raw scores are provided in the ```outputs``` column. For the ```property``` column, the value ```prevoius_link``` denotes **moral relevance** and the value ```polarity``` denotes **moral polarity**.



#### External datasets
You need to download the following datasets in order to reproduce all our results. 

The Corpus of Historical American English (COHA) is available at (https://www.english-corpora.org/coha/). 

The New York Times annotated corpus is available at (https://huggingface.co/datasets/irds/nyt). 

The Moral Foundations Dictionary available at (https://osf.io/ezn37/). 

The English word association network (SWOW) can be accessed through (https://smallworldofwords.org/en/project/research), and the moral association scores are available at the ```data/SWOWEN``` directory of this repository.

Disease and political terms can be found in the ```data/SWOWEN``` directory of this repository. 


The list of popular names in the United States is available on the Social Security Administration website at (https://www.ssa.gov/oact/babynames/limits.html). 

The Historical Conflict Event Dataset can be accessed through (https://journals.sagepub.com/doi/10.1177/00220027221119085).

Conceptual category norm dataset is available at (https://osf.io/jgcu6/). 

Average price dataset is available at the U.S. Bureau of Labor Statistics website (https://www.bls.gov/cpi/factsheets/average-prices.htm).

Congressional speech record dataset is available at (https://data.stanford.edu/congress_text). 

*Gallup moral survey series can be accessed through Gallup Analytics service (https://www.gallup.com/analytics/214565/universities-colleges-using-gallup-analytics.aspx). 

\* Restricted access


#### Installation

You need ```Python 3.10.13``` with the following dependencies.

```
beautifulsoup4==4.13.3
en_core_web_sm==3.7.0
matplotlib==3.7.3
networkx==3.1
nltk==3.8.1
numpy==1.25.2
openai==1.66.3
pandas==2.2.3
PyYAML==6.0.1
PyYAML==6.0.2
scikit_learn==1.3.1
scipy==1.15.2
seaborn==0.13.2
spacy==3.7.2
statsmodels==0.14.0
torch==2.2.1
torch_geometric==2.6.1
tqdm==4.66.1
transformers==4.46.0.dev0
wikipedia==1.4.0
adjustText==1.2.0
scikit_learn==1.3.1
scipy==1.14.1
```

#### Experiments

```notebooks/SWOW_prediction_coha.ipynb``` shows our evaluation performance on COHA.



```notebooks/SWOW_prediction_nyt.ipynb``` shows our evaluation performance on NYT.

```notebooks/fig1.ipynb``` reproduces display item 1a.

```notebooks/display_item_eval.ipynb``` reproduces display items 2a and 2b.


```notebooks/display_item_2.ipynb``` reproduces display items 2c and 2d.


```notebooks/battle.ipynb``` compares moral scores (moral relevance and moral polarity) before and after international wars and conflicts (display items 2e and 2f).





```notebooks/data_size.ipynb``` shows the number of concepts in COHA and NYT datasets.

```notebooks/change_detection.ipynb``` identfies concepts with the sharpest change in moral relevance and moral polarity scores over time.


```notebooks/coha_category_eval.ipynb``` shows our results for moralization in conceptual categories.


```notebooks/coha_correlation.ipynb``` computes pair-wise correlation in different conceptual categories.

```notebooks/congress_speech.ipynb``` predicts congressional speech frequencies based on moral scores.

```notebooks/nyt_gallup.ipynb``` compares Gallup moral ratings with model generated ones from NYT.

```notebooks/nyt_us_election.ipynb``` shows moral scores with respect to US presidential election cycles.

```notebooks/price_analysis.ipynb``` compares moral scores with retail product prices.


```notebooks/emnlp_prediction.ipynb``` and ```notebooks/emotion_baseline.ipynb```, and ```notebooks/word2vec_dieases.ipynb```, and
```notebooks/xlm_eval.ipynb``` provide comparison against different baselines.


#### Scripts

If you wish to generate new moral association scores using alternative textual corpora, add appropriate data reading functions to the ```get_data``` function inside ```SWOW_prediction/data_preprocessing.py```, and modify the scripts ```scripts/data_job_coha.sh```, and ```scripts/historical_inference.sh``` accordingly. 

If you wish to train a new model based on your custom textual data, you should also modify and run the ```scripts/training_job.sh``` script. Otherwise, this script will reproduce our models for COHA and NYT datasets.

Other configurations can be modified in ```SWOW_prediction/config_features.yml```.

Run ```scripts/evaluation_job.sh``` and ```scripts/historical_inference.sh``` after training to apply the models on contemporary and historical textual corpora. 

#### Demo visualization
https://warz.shinyapps.io/MoralityVisualizer/.

```notebooks/fig1.ipynb``` reproduces display item 1a, and can be modified to explore other concepts (expected time to run: 1 minute).


